# ðŸ›¡ï¸ **Secure Offline RAG System**

We built a RAG system which runs locally on cpu in an offline mode. It uses open source large language models for performing retrieval augmented generation. 

## ðŸš€ **Tech Stack**
### **Programming Language**
#### ðŸ Python
### **Frameworks & Libraries**
#### ðŸŽ¨ Streamlit â€” For building the interactive and intuitive user interface.
#### ðŸ”— Langchain â€” To streamline and optimize the RAG pipeline.
#### ðŸ§  Ollama â€” Efficient, local LLM deployment for high-quality inference.
#### ðŸ¤— Hugging Face â€” Powerful models and tools for natural language processing.
### **Vector Database**
#### ðŸ” FAISS (Facebook AI Similarity Search) â€” Fast, efficient, and scalable vector search for document retrieval.
### **Reranking Model**
#### ðŸŽ¯ BAAI/bge-reranker-base â€” Advanced model for reranking results to ensure relevant and accurate information is returned.

## âœ¨ **Features** 

- Minimum CPU memory and RAM usage
- Runs locally even in an offline environment (For PDFs and other documents)
- Highly efficient and quantized model
- Multilingual support with over 29 languages including Chinese
- Fast inference
- Intuitive UI
- Add new documents to the system without the need for a complete reindexing process, ensuring dynamic and flexible integration of new knowledge.
 - Built with a focus on minimizing memory usage, the system leverages lightweight retrieval techniques such as FAISS (or alternatives like inverted indices) to manage large datasets without consuming excessive memory.
- Low Latency

## ðŸ“‚ **File Structure**
![Screenshot 2024-11-15 150722](https://github.com/user-attachments/assets/ad5bf8bf-634a-477d-aee0-d304026b69ae)


## ðŸ› ï¸ **Installation Steps**

### Clone the repository: 

```Python
> git clone https://github.com/ParamThakkar123/Secure-Local-Offline-Rag-System.git
```
### Change directory: 
```Python
> cd Secure-Local-Offline-Rag-System
```
### Installl the dependencies : 
```Python
> pip install -r requirements.txt
```
Download Ollama app and run it

### Open command line and type : 
```Python
> ollama pull qwen2:0.5b-instruct-q3_K_S
> ollama pull nextfire/paraphrase-multilingual-minilm:l12-v2
```


### Run the app.py using the following command in the command line
```Python
> streamlit run app.py
```

### If the above command gives the error â€œstreamlit not recognizedâ€, enter the following command
```Python
> python -m streamlit run app.py
```
## ðŸ“¸ **Output Screenshots**
![Screenshot 2024-11-15 170214](https://github.com/user-attachments/assets/14558b74-f65b-460f-9ec7-1e16b6bf52b9)


![Screenshot 2024-11-15 182035](https://github.com/user-attachments/assets/6d45e6f0-cd23-47c3-87d8-7ea1aaf3de85)

## ðŸŽ¥ **Demo Video**
https://github.com/user-attachments/assets/b825dcbd-7965-4ba1-ae37-a6f0fce77aad

